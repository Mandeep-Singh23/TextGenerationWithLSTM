{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xVxtwGEw6_-z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lizzr5r_7tYD",
        "outputId": "5fb12662-8ea2-4ee7-daf3-91f63e40790c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XfaWW7qt7_YJ"
      },
      "outputs": [],
      "source": [
        "!cp /mydrive/Sequence\\ Learning/Text_Generation_using_LSTM/wonderland.txt /content\n",
        "#weights on drive\n",
        "#!cp /mydrive/Sequence\\ Learning/Text_Generation_using_LSTM/weights-improvment-20-1.9774.hdf5 /content\n",
        "#!cp /mydrive/Sequence\\ Learning/Text_Generation_using_LSTM/weights-improvement-bigger-40-1.2682.hdf5 /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4GhgnepE8JC6"
      },
      "outputs": [],
      "source": [
        "filename = 'wonderland.txt'\n",
        "raw_text = open(filename, 'r',encoding = 'utf-8').read()\n",
        "raw_text = raw_text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xK6cP2u8KIu",
        "outputId": "b2abad63-5864-4356-f276-920ba370e759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ing to get very tired of sitting by her sister on \n"
          ]
        }
      ],
      "source": [
        "print(raw_text[50:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HrL6vqDoTB70"
      },
      "outputs": [],
      "source": [
        "#Convertinv Character into interger for the model.\n",
        "chars = sorted(list(set(raw_text)))\n",
        "chars_to_int = dict((c,i) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OZCsMRJTaSI",
        "outputId": "28481ac2-b07c-402d-ea58-a7f09d07ec1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '(': 3, ')': 4, '*': 5, ',': 6, '-': 7, '.': 8, ':': 9, ';': 10, '?': 11, '[': 12, ']': 13, '_': 14, 'a': 15, 'b': 16, 'c': 17, 'd': 18, 'e': 19, 'f': 20, 'g': 21, 'h': 22, 'i': 23, 'j': 24, 'k': 25, 'l': 26, 'm': 27, 'n': 28, 'o': 29, 'p': 30, 'q': 31, 'r': 32, 's': 33, 't': 34, 'u': 35, 'v': 36, 'w': 37, 'x': 38, 'y': 39, 'z': 40, 'ù': 41, '—': 42, '‘': 43, '’': 44, '“': 45, '”': 46}\n"
          ]
        }
      ],
      "source": [
        "print(chars_to_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "5X1poeLcTeTz",
        "outputId": "b58f52c1-6e3b-4c00-c3b9-4dd5ec4d00bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Character  Integer\n",
              "0        \\n        0\n",
              "1                  1\n",
              "2         !        2\n",
              "3         (        3\n",
              "4         )        4\n",
              "5         *        5\n",
              "6         ,        6\n",
              "7         -        7\n",
              "8         .        8\n",
              "9         :        9"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-81fa9377-b3ee-4498-ba28-1a798f521ef4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Character</th>\n",
              "      <th>Integer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\n</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>!</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>)</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>*</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>,</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>.</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>:</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81fa9377-b3ee-4498-ba28-1a798f521ef4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-81fa9377-b3ee-4498-ba28-1a798f521ef4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-81fa9377-b3ee-4498-ba28-1a798f521ef4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a7ab281d-267a-4dda-bc19-a98d3fd259c7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7ab281d-267a-4dda-bc19-a98d3fd259c7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a7ab281d-267a-4dda-bc19-a98d3fd259c7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 47,\n  \"fields\": [\n    {\n      \"column\": \"Character\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 47,\n        \"samples\": [\n          \"m\",\n          \"y\",\n          \"l\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Integer\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 0,\n        \"max\": 46,\n        \"num_unique_values\": 47,\n        \"samples\": [\n          27,\n          39,\n          26\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "#Using Dataframe to see the integer assigned to characters\n",
        "df = pd.DataFrame(list(chars_to_int.items()), columns=['Character', 'Integer'])\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MbfOKUUUbrt",
        "outputId": "586ed102-2270-48a4-ecdd-fade657b7255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Character: 144017\n",
            "Total Vaacb: 47\n"
          ]
        }
      ],
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Character:\",n_chars)\n",
        "print(\"Total Vaacb:\",n_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Around 150K characters are present and 47 different characters in the whole data."
      ],
      "metadata": {
        "id": "APPDuYLv_gV3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-5B8LEOViJj",
        "outputId": "e2ded827-76d5-4721-d3bc-8928d7be5290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns: 143917\n"
          ]
        }
      ],
      "source": [
        "#Preparing the Training Data\n",
        "seq_length = 100\n",
        "data_X = []\n",
        "data_y = []\n",
        "for i in range(0,n_chars - seq_length,1):\n",
        "  seq_in = raw_text[i : i+seq_length]\n",
        "  seq_out = raw_text[i+seq_length]\n",
        "  data_X.append([chars_to_int[char] for char in seq_in])\n",
        "  data_y.append(chars_to_int[seq_out])\n",
        "n_patterns = len(data_X)\n",
        "print(\"Total Patterns:\",n_patterns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "nL56-yoPX4MO"
      },
      "outputs": [],
      "source": [
        "#rescaling the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network\n",
        "#using the sigmoid activation function by default.\n",
        "X = np.reshape(data_X,(n_patterns, seq_length,1))\n",
        "X = X/float(n_vocab)\n",
        "y = to_categorical(data_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Ncn6e-lCZxST"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256,input_shape = (X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1],activation = 'softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "P4uRTWHRbBoV"
      },
      "outputs": [],
      "source": [
        "filepath = 'weights-improvment-{epoch:02d}-{loss:.4f}.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'loss',save_best_only=True, verbose = 1,mode = 'min')\n",
        "callback_list = [checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDQ18RaVcQby",
        "outputId": "e5f4c862-c814-4bcc-d893-b33b74e2ab1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1121/1125 [============================>.] - ETA: 0s - loss: 3.0288\n",
            "Epoch 1: loss improved from inf to 3.02845, saving model to weights-improvment-01-3.0284.hdf5\n",
            "1125/1125 [==============================] - 17s 13ms/step - loss: 3.0284\n",
            "Epoch 2/20\n",
            "  11/1125 [..............................] - ETA: 13s - loss: 2.8831"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1123/1125 [============================>.] - ETA: 0s - loss: 2.8074\n",
            "Epoch 2: loss improved from 3.02845 to 2.80736, saving model to weights-improvment-02-2.8074.hdf5\n",
            "1125/1125 [==============================] - 14s 12ms/step - loss: 2.8074\n",
            "Epoch 3/20\n",
            "1124/1125 [============================>.] - ETA: 0s - loss: 2.6887\n",
            "Epoch 3: loss improved from 2.80736 to 2.68873, saving model to weights-improvment-03-2.6887.hdf5\n",
            "1125/1125 [==============================] - 14s 13ms/step - loss: 2.6887\n",
            "Epoch 4/20\n",
            "1124/1125 [============================>.] - ETA: 0s - loss: 2.6072\n",
            "Epoch 4: loss improved from 2.68873 to 2.60716, saving model to weights-improvment-04-2.6072.hdf5\n",
            "1125/1125 [==============================] - 14s 13ms/step - loss: 2.6072\n",
            "Epoch 5/20\n",
            "1123/1125 [============================>.] - ETA: 0s - loss: 2.5391\n",
            "Epoch 5: loss improved from 2.60716 to 2.53907, saving model to weights-improvment-05-2.5391.hdf5\n",
            "1125/1125 [==============================] - 15s 13ms/step - loss: 2.5391\n",
            "Epoch 6/20\n",
            "1124/1125 [============================>.] - ETA: 0s - loss: 2.4775\n",
            "Epoch 6: loss improved from 2.53907 to 2.47760, saving model to weights-improvment-06-2.4776.hdf5\n",
            "1125/1125 [==============================] - 15s 13ms/step - loss: 2.4776\n",
            "Epoch 7/20\n",
            "1124/1125 [============================>.] - ETA: 0s - loss: 2.4263\n",
            "Epoch 7: loss improved from 2.47760 to 2.42643, saving model to weights-improvment-07-2.4264.hdf5\n",
            "1125/1125 [==============================] - 15s 13ms/step - loss: 2.4264\n",
            "Epoch 8/20\n",
            "1122/1125 [============================>.] - ETA: 0s - loss: 2.3791\n",
            "Epoch 8: loss improved from 2.42643 to 2.37897, saving model to weights-improvment-08-2.3790.hdf5\n",
            "1125/1125 [==============================] - 15s 13ms/step - loss: 2.3790\n",
            "Epoch 9/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 2.3457\n",
            "Epoch 9: loss improved from 2.37897 to 2.34566, saving model to weights-improvment-09-2.3457.hdf5\n",
            "1125/1125 [==============================] - 15s 13ms/step - loss: 2.3457\n",
            "Epoch 10/20\n",
            "1121/1125 [============================>.] - ETA: 0s - loss: 2.3041\n",
            "Epoch 10: loss improved from 2.34566 to 2.30382, saving model to weights-improvment-10-2.3038.hdf5\n",
            "1125/1125 [==============================] - 14s 13ms/step - loss: 2.3038\n",
            "Epoch 11/20\n",
            "1123/1125 [============================>.] - ETA: 0s - loss: 2.2625\n",
            "Epoch 11: loss improved from 2.30382 to 2.26243, saving model to weights-improvment-11-2.2624.hdf5\n",
            "1125/1125 [==============================] - 15s 13ms/step - loss: 2.2624\n",
            "Epoch 12/20\n",
            "1123/1125 [============================>.] - ETA: 0s - loss: 2.2274\n",
            "Epoch 12: loss improved from 2.26243 to 2.22743, saving model to weights-improvment-12-2.2274.hdf5\n",
            "1125/1125 [==============================] - 15s 13ms/step - loss: 2.2274\n",
            "Epoch 13/20\n",
            "1121/1125 [============================>.] - ETA: 0s - loss: 2.1947\n",
            "Epoch 13: loss improved from 2.22743 to 2.19496, saving model to weights-improvment-13-2.1950.hdf5\n",
            "1125/1125 [==============================] - 15s 13ms/step - loss: 2.1950\n",
            "Epoch 14/20\n",
            "1121/1125 [============================>.] - ETA: 0s - loss: 2.1602\n",
            "Epoch 14: loss improved from 2.19496 to 2.15991, saving model to weights-improvment-14-2.1599.hdf5\n",
            "1125/1125 [==============================] - 14s 13ms/step - loss: 2.1599\n",
            "Epoch 15/20\n",
            "1124/1125 [============================>.] - ETA: 0s - loss: 2.1277\n",
            "Epoch 15: loss improved from 2.15991 to 2.12773, saving model to weights-improvment-15-2.1277.hdf5\n",
            "1125/1125 [==============================] - 15s 13ms/step - loss: 2.1277\n",
            "Epoch 16/20\n",
            "1122/1125 [============================>.] - ETA: 0s - loss: 2.0940\n",
            "Epoch 16: loss improved from 2.12773 to 2.09381, saving model to weights-improvment-16-2.0938.hdf5\n",
            "1125/1125 [==============================] - 14s 13ms/step - loss: 2.0938\n",
            "Epoch 17/20\n",
            "1122/1125 [============================>.] - ETA: 0s - loss: 2.0627\n",
            "Epoch 17: loss improved from 2.09381 to 2.06278, saving model to weights-improvment-17-2.0628.hdf5\n",
            "1125/1125 [==============================] - 15s 13ms/step - loss: 2.0628\n",
            "Epoch 18/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 2.0331\n",
            "Epoch 18: loss improved from 2.06278 to 2.03306, saving model to weights-improvment-18-2.0331.hdf5\n",
            "1125/1125 [==============================] - 15s 13ms/step - loss: 2.0331\n",
            "Epoch 19/20\n",
            "1122/1125 [============================>.] - ETA: 0s - loss: 2.0047\n",
            "Epoch 19: loss improved from 2.03306 to 2.00466, saving model to weights-improvment-19-2.0047.hdf5\n",
            "1125/1125 [==============================] - 15s 13ms/step - loss: 2.0047\n",
            "Epoch 20/20\n",
            "1124/1125 [============================>.] - ETA: 0s - loss: 1.9775\n",
            "Epoch 20: loss improved from 2.00466 to 1.97739, saving model to weights-improvment-20-1.9774.hdf5\n",
            "1125/1125 [==============================] - 14s 13ms/step - loss: 1.9774\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fb63b1b8c10>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "model.fit(X,y,epochs = 20, batch_size = 128, callbacks = callback_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "BDBQJa1_choC"
      },
      "outputs": [],
      "source": [
        "filename = 'weights-improvment-20-1.9774.hdf5'\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy',optimizer = 'adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "KyGGesXIfVTq"
      },
      "outputs": [],
      "source": [
        "int_to_char = dict((i,c) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3P61IBmfdAa",
        "outputId": "f8964c9f-e3d2-4db0-d9bd-783525c5010b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" aying anything more till the pigeon had finished.\n",
            "\n",
            "“as if it wasn’t trouble enough hatching the eggs \"\n"
          ]
        }
      ],
      "source": [
        "start = np.random.randint(0,len(data_X)-1)\n",
        "pattern = data_X[start]\n",
        "print('Seed:')\n",
        "print('\\\"',''.join([int_to_char[value] for value in pattern]), '\\\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lua4I-eVgSXK",
        "outputId": "840a433c-6650-486d-b7af-760ffdef68ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ee of thens ” said alice an an cxplyuul \n",
            "“oh  o aaner the mirtle gorse      beautirui      wou              fot in wou coo a foea tf ther.”\n",
            "\n",
            "“i wooher wo tey i woole if the mors ou toaee”!” she monk turtle senlied torh a aaterplll\n",
            "tene an i sinele and the had foo a lotte, and the woid ti the whrle thrne  the kad here to the toed a deel of theng sas an in afdine the was oo a lirtle oo the that whre a latge carl was anl aroiers, \n",
            "“hh mo hin hor then ”our eajesn,” said the caterpillar.\n",
            "\n",
            "“ie iou the more tu here th the soin ”oul toen ” she said to herself, “io wou don’t keke to moke the mort oo the reaten.”\n",
            "\n",
            "“i whnu h tound ”ou soon!” said the caterpillar.\n",
            "\n",
            "“he moeh tonh yhu io wou can ” said the mors turtle. \n",
            "“whel io horsten ” she mock turtle sepeied an an offended tone. \n",
            "“ie ious bn a moee tare,” she hatter went on an an offende tone. \n",
            "“ie iou the more tu seat to toe toin of the soise ”hu,” shi said to herself, “and io the would wou doul toe toin a crea fo soen a doeat outee, and io ii \n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "for i in range(1000):\n",
        "  X = np.reshape(pattern, (1,len(pattern),1))\n",
        "  X = X/float(n_vocab)\n",
        "  prediction = model.predict(X,verbose = 0)\n",
        "  index = np.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  sys.stdout.write(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Model has tried predicting but it's not as we expected."
      ],
      "metadata": {
        "id": "9G6JzmHjLbY1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl4e57P9l0Le"
      },
      "source": [
        "#Larger LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "KUJrZhZtksJo"
      },
      "outputs": [],
      "source": [
        "model1 = Sequential()\n",
        "model1.add(LSTM(256,input_shape=(X.shape[1],X.shape[2]),return_sequences=True))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(LSTM(256))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(Dense(y.shape[1],activation = 'softmax'))\n",
        "model1.compile(loss = 'categorical_crossentropy',optimizer = 'adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Po1uHExHme05"
      },
      "outputs": [],
      "source": [
        "filepath = 'weights-improvement-bigger-{epoch:02d}-{loss:.4f}.hdf5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "H6263ivDnMJ4"
      },
      "outputs": [],
      "source": [
        "filepath = 'weights-improvement-bigger-{epoch:02d}-{loss:.4f}.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'loss',save_best_only=True, verbose = 1,mode = 'min')\n",
        "callback_list = [checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sINVvBosnYLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8596864-f91a-434d-b420-3b5d77f38a7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 2.8083\n",
            "Epoch 1: loss improved from inf to 2.80824, saving model to weights-improvement-bigger-01-2.8082.hdf5\n",
            "2249/2249 [==============================] - 46s 18ms/step - loss: 2.8082\n",
            "Epoch 2/50\n",
            "   7/2249 [..............................] - ETA: 39s - loss: 2.5501"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2248/2249 [============================>.] - ETA: 0s - loss: 2.3853\n",
            "Epoch 2: loss improved from 2.80824 to 2.38519, saving model to weights-improvement-bigger-02-2.3852.hdf5\n",
            "2249/2249 [==============================] - 40s 18ms/step - loss: 2.3852\n",
            "Epoch 3/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 2.1794\n",
            "Epoch 3: loss improved from 2.38519 to 2.17956, saving model to weights-improvement-bigger-03-2.1796.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 2.1796\n",
            "Epoch 4/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 2.0511\n",
            "Epoch 4: loss improved from 2.17956 to 2.05096, saving model to weights-improvement-bigger-04-2.0510.hdf5\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 2.0510\n",
            "Epoch 5/50\n",
            "2249/2249 [==============================] - ETA: 0s - loss: 1.9567\n",
            "Epoch 5: loss improved from 2.05096 to 1.95671, saving model to weights-improvement-bigger-05-1.9567.hdf5\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.9567\n",
            "Epoch 6/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 1.8857\n",
            "Epoch 6: loss improved from 1.95671 to 1.88564, saving model to weights-improvement-bigger-06-1.8856.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.8856\n",
            "Epoch 7/50\n",
            "2249/2249 [==============================] - ETA: 0s - loss: 1.8292\n",
            "Epoch 7: loss improved from 1.88564 to 1.82923, saving model to weights-improvement-bigger-07-1.8292.hdf5\n",
            "2249/2249 [==============================] - 42s 18ms/step - loss: 1.8292\n",
            "Epoch 8/50\n",
            "2249/2249 [==============================] - ETA: 0s - loss: 1.7753\n",
            "Epoch 8: loss improved from 1.82923 to 1.77530, saving model to weights-improvement-bigger-08-1.7753.hdf5\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.7753\n",
            "Epoch 9/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 1.7326\n",
            "Epoch 9: loss improved from 1.77530 to 1.73265, saving model to weights-improvement-bigger-09-1.7327.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.7327\n",
            "Epoch 10/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.6944\n",
            "Epoch 10: loss improved from 1.73265 to 1.69445, saving model to weights-improvement-bigger-10-1.6945.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.6945\n",
            "Epoch 11/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 1.6586\n",
            "Epoch 11: loss improved from 1.69445 to 1.65859, saving model to weights-improvement-bigger-11-1.6586.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.6586\n",
            "Epoch 12/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 1.6282\n",
            "Epoch 12: loss improved from 1.65859 to 1.62810, saving model to weights-improvement-bigger-12-1.6281.hdf5\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.6281\n",
            "Epoch 13/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.5947\n",
            "Epoch 13: loss improved from 1.62810 to 1.59464, saving model to weights-improvement-bigger-13-1.5946.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.5946\n",
            "Epoch 14/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 1.5725\n",
            "Epoch 14: loss improved from 1.59464 to 1.57247, saving model to weights-improvement-bigger-14-1.5725.hdf5\n",
            "2249/2249 [==============================] - 42s 18ms/step - loss: 1.5725\n",
            "Epoch 15/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.5454\n",
            "Epoch 15: loss improved from 1.57247 to 1.54542, saving model to weights-improvement-bigger-15-1.5454.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.5454\n",
            "Epoch 16/50\n",
            "2249/2249 [==============================] - ETA: 0s - loss: 1.5214\n",
            "Epoch 16: loss improved from 1.54542 to 1.52139, saving model to weights-improvement-bigger-16-1.5214.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.5214\n",
            "Epoch 17/50\n",
            "2249/2249 [==============================] - ETA: 0s - loss: 1.5010\n",
            "Epoch 17: loss improved from 1.52139 to 1.50104, saving model to weights-improvement-bigger-17-1.5010.hdf5\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.5010\n",
            "Epoch 18/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.4808\n",
            "Epoch 18: loss improved from 1.50104 to 1.48077, saving model to weights-improvement-bigger-18-1.4808.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.4808\n",
            "Epoch 19/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 1.4636\n",
            "Epoch 19: loss improved from 1.48077 to 1.46362, saving model to weights-improvement-bigger-19-1.4636.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.4636\n",
            "Epoch 20/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 1.4433\n",
            "Epoch 20: loss improved from 1.46362 to 1.44317, saving model to weights-improvement-bigger-20-1.4432.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.4432\n",
            "Epoch 21/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.4304\n",
            "Epoch 21: loss improved from 1.44317 to 1.43040, saving model to weights-improvement-bigger-21-1.4304.hdf5\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.4304\n",
            "Epoch 22/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.4131\n",
            "Epoch 22: loss improved from 1.43040 to 1.41318, saving model to weights-improvement-bigger-22-1.4132.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.4132\n",
            "Epoch 23/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 1.3990\n",
            "Epoch 23: loss improved from 1.41318 to 1.39896, saving model to weights-improvement-bigger-23-1.3990.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.3990\n",
            "Epoch 24/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.3889\n",
            "Epoch 24: loss improved from 1.39896 to 1.38896, saving model to weights-improvement-bigger-24-1.3890.hdf5\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.3890\n",
            "Epoch 25/50\n",
            "2249/2249 [==============================] - ETA: 0s - loss: 1.3752\n",
            "Epoch 25: loss improved from 1.38896 to 1.37524, saving model to weights-improvement-bigger-25-1.3752.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.3752\n",
            "Epoch 26/50\n",
            "2249/2249 [==============================] - ETA: 0s - loss: 1.3619\n",
            "Epoch 26: loss improved from 1.37524 to 1.36186, saving model to weights-improvement-bigger-26-1.3619.hdf5\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.3619\n",
            "Epoch 27/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.3553\n",
            "Epoch 27: loss improved from 1.36186 to 1.35537, saving model to weights-improvement-bigger-27-1.3554.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.3554\n",
            "Epoch 28/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 1.3464\n",
            "Epoch 28: loss improved from 1.35537 to 1.34650, saving model to weights-improvement-bigger-28-1.3465.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.3465\n",
            "Epoch 29/50\n",
            "2249/2249 [==============================] - ETA: 0s - loss: 1.3332\n",
            "Epoch 29: loss improved from 1.34650 to 1.33323, saving model to weights-improvement-bigger-29-1.3332.hdf5\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.3332\n",
            "Epoch 30/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 1.3299\n",
            "Epoch 30: loss improved from 1.33323 to 1.32993, saving model to weights-improvement-bigger-30-1.3299.hdf5\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.3299\n",
            "Epoch 31/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.3210\n",
            "Epoch 31: loss improved from 1.32993 to 1.32101, saving model to weights-improvement-bigger-31-1.3210.hdf5\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.3210\n",
            "Epoch 32/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.3099\n",
            "Epoch 32: loss improved from 1.32101 to 1.31002, saving model to weights-improvement-bigger-32-1.3100.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.3100\n",
            "Epoch 33/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 1.3041\n",
            "Epoch 33: loss improved from 1.31002 to 1.30419, saving model to weights-improvement-bigger-33-1.3042.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.3042\n",
            "Epoch 34/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.2954\n",
            "Epoch 34: loss improved from 1.30419 to 1.29544, saving model to weights-improvement-bigger-34-1.2954.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.2954\n",
            "Epoch 35/50\n",
            "2249/2249 [==============================] - ETA: 0s - loss: 1.2885\n",
            "Epoch 35: loss improved from 1.29544 to 1.28846, saving model to weights-improvement-bigger-35-1.2885.hdf5\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.2885\n",
            "Epoch 36/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 1.2862\n",
            "Epoch 36: loss improved from 1.28846 to 1.28626, saving model to weights-improvement-bigger-36-1.2863.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.2863\n",
            "Epoch 37/50\n",
            "2249/2249 [==============================] - ETA: 0s - loss: 1.2782\n",
            "Epoch 37: loss improved from 1.28626 to 1.27821, saving model to weights-improvement-bigger-37-1.2782.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.2782\n",
            "Epoch 38/50\n",
            "2249/2249 [==============================] - ETA: 0s - loss: 1.2748\n",
            "Epoch 38: loss improved from 1.27821 to 1.27478, saving model to weights-improvement-bigger-38-1.2748.hdf5\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.2748\n",
            "Epoch 39/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.2682\n",
            "Epoch 39: loss improved from 1.27478 to 1.26828, saving model to weights-improvement-bigger-39-1.2683.hdf5\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.2683\n",
            "Epoch 40/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.2682\n",
            "Epoch 40: loss improved from 1.26828 to 1.26815, saving model to weights-improvement-bigger-40-1.2682.hdf5\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.2682\n",
            "Epoch 41/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.8393\n",
            "Epoch 41: loss did not improve from 1.26815\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.8400\n",
            "Epoch 42/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 2.6633\n",
            "Epoch 42: loss did not improve from 1.26815\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 2.6634\n",
            "Epoch 43/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 2.3936\n",
            "Epoch 43: loss did not improve from 1.26815\n",
            "2249/2249 [==============================] - 42s 18ms/step - loss: 2.3937\n",
            "Epoch 44/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 2.1831\n",
            "Epoch 44: loss did not improve from 1.26815\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 2.1831\n",
            "Epoch 45/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 2.0221\n",
            "Epoch 45: loss did not improve from 1.26815\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 2.0221\n",
            "Epoch 46/50\n",
            "2249/2249 [==============================] - ETA: 0s - loss: 1.9115\n",
            "Epoch 46: loss did not improve from 1.26815\n",
            "2249/2249 [==============================] - 42s 18ms/step - loss: 1.9115\n",
            "Epoch 47/50\n",
            "2249/2249 [==============================] - ETA: 0s - loss: 1.8278\n",
            "Epoch 47: loss did not improve from 1.26815\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.8278\n",
            "Epoch 48/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 1.7628\n",
            "Epoch 48: loss did not improve from 1.26815\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.7628\n",
            "Epoch 49/50\n",
            "2247/2249 [============================>.] - ETA: 0s - loss: 1.7107\n",
            "Epoch 49: loss did not improve from 1.26815\n",
            "2249/2249 [==============================] - 42s 19ms/step - loss: 1.7106\n",
            "Epoch 50/50\n",
            "2248/2249 [============================>.] - ETA: 0s - loss: 1.6685\n",
            "Epoch 50: loss did not improve from 1.26815\n",
            "2249/2249 [==============================] - 41s 18ms/step - loss: 1.6685\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fb6357a55a0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model1.fit(X,y,epochs = 50,batch_size=64, callbacks = callback_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "FNL8HK5dpeEO"
      },
      "outputs": [],
      "source": [
        "#Selecting weights will lowest loss.\n",
        "filename = 'weights-improvement-bigger-40-1.2682.hdf5'\n",
        "model1.load_weights(filename)\n",
        "model1.compile(loss='categorical_crossentropy',optimizer = 'adam')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i,c) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "Tz3ugTmZzU-7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = np.random.randint(0,len(data_X)-1)\n",
        "pattern = data_X[start]\n",
        "print('Seed:')\n",
        "print('\\\"',''.join([int_to_char[value] for value in pattern]), '\\\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzyU_0fYzdt5",
        "outputId": "17b20abe-30cf-4127-a431-9067282f561d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" e i’ve got to?” (alice had no\n",
            "idea what latitude was, or longitude either, but thought they were nic \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "  X = np.reshape(pattern, (1,len(pattern),1))\n",
        "  X = X/float(n_vocab)\n",
        "  prediction = model1.predict(X,verbose = 0)\n",
        "  index = np.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  sys.stdout.write(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpmnOnkIzlV5",
        "outputId": "0e72f664-8cff-4824-8c91-02cca1d4d03b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e all of the laster and the larch hare was seeling at the court, and was going to see it all the table for tome minute the thought it was a little siarp her hands, and she was a little siarp her hands and then she was all to tpeak as the sabbit, and she was soon for tome minutes, and the thought it was oot a little siarp her and then, and then was sitting on the door, and the white rabbit were all talking at the court, and was going to see it all the time, and then was she was seriing things at the white rabbit as well as she could, and the tornd of the lock turtle said in a low voice,\n",
            "“yhat a canlya whine wou fan any muster to be a grown of the sea.”\n",
            "\n",
            "“i don’t know what i say it out of the sorgs!” said the king, \n",
            "“then the queen was the eeged the shat then the wery sathen of the sea,” the mock turtle seplied rizinl, and the white rabbit were all talking at the court, and was going to see it all the table for tome minute the thought it was a little siarp her hands, and she was a little\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparatively better result this time."
      ],
      "metadata": {
        "id": "KdCT-VGNLDn2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}